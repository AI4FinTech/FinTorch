{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "In this hands-on Python tutorial, we'll delve into an intriguing dataset that captures the dynamics of transactions within a blockchain network. Our goal will be to prepare this data for training machine learning models using the powerful PyTorch and PyTorch Lightning frameworks.\n",
    "\n",
    "# The Dataset\n",
    "\n",
    "Let's dissect the dataset's description:\n",
    "\n",
    "* Temporal Structure: The dataset is divided into 49 distinct time steps, each spaced roughly two weeks apart. Within every time step, we find a connected group of transactions occurring within a three-hour window.\n",
    "* Transaction Features: Each transaction is characterized by 94 'local' features. These include its timestamp, input/output counts, fees, volume, and interesting aggregations (e.g., average BTC involved in inputs/outputs).\n",
    "* Neighborhood Features: An additional 72 'aggregated' features illuminate each transaction's context. We get statistics like the maximum, minimum, standard deviation, and correlation coefficients derived from transactions one hop away.\n",
    "\n",
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "We'll begin our journey with exploratory data analysis (EDA). Key things to explore:\n",
    "\n",
    "* Distributions: Examine the distributions of transaction features (fees, volumes, etc.) to spot patterns and potential outliers.\n",
    "* Correlations: Investigate relationships between transaction features. Which features correlate, and can this insight inform our model design?\n",
    "* Temporal Trends: Analyze how features change across the 49 time steps. Are there seasonal effects or evolving network behaviors?\n",
    "\n",
    "# PyTorch Datasets\n",
    "\n",
    "Our EDA findings will guide how we structure our PyTorch Datasets. Here's where things get exciting:\n",
    "\n",
    "* Custom Dataset Class: We'll create a custom PyTorch Dataset class to load and preprocess the raw data dynamically during model training.\n",
    "* Data Transformations: We might apply scaling, normalization, or other essential transformations to make the data more suitable for machine learning.\n",
    "\n",
    "# PyTorch Lightning Integration\n",
    "\n",
    "Finally, we'll leverage PyTorch Lightning to streamline our training process.\n",
    "\n",
    "* DataModule: A PyTorch Lightning DataModule will encapsulate our Datasets, manage data loading, and handle batching for efficient model training.\n",
    "\n",
    "# What You'll Build\n",
    "\n",
    "By the end of this tutorial, you'll have a solid foundation for training machine learning models on this dataset. This foundation sets the stage for exciting applications such as:\n",
    "\n",
    "* Fraud detection\n",
    "* Transaction pattern analysis\n",
    "* Blockchain network behavior prediction\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset background\n",
    "The dataset description originates from [Kaggle Elliptic Data Set](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set) and is restated here for convenience. \n",
    "\n",
    "## Dataset description\n",
    "This anonymized data set is a transaction graph collected from the Bitcoin blockchain. A node in the graph represents a transaction, an edge can be viewed as a flow of Bitcoins between one transaction and the other. Each node has 166 features and has been labeled as being created by a \"licit\", \"illicit\" or \"unknown\" entity.\n",
    "\n",
    "### Nodes and edges\n",
    "\n",
    "The graph is made of 203,769 nodes and 234,355 edges. Two percent (4,545) of the nodes are labelled class1 (illicit). Twenty-one percent (42,019) are labelled class2 (licit). The remaining transactions are not labelled with regard to licit versus illicit.\n",
    "\n",
    "### Features\n",
    "\n",
    "There are 166 features associated with each node. Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset. There is a time step associated to each node, representing a measure of the time when a transaction was broadcasted to the Bitcoin network. The time steps, running from 1 to 49, are evenly spaced with an interval of about two weeks. Each time step contains a single connected component of transactions that appeared on the blockchain within less than three hours between each other; there are no edges connecting the different time steps.\n",
    "\n",
    "The first 94 features represent local information about the transaction – including the time step described above, number of inputs/outputs, transaction fee, output volume and aggregated figures such as average BTC received (spent) by the inputs/outputs and average number of incoming (outgoing) transactions associated with the inputs/outputs. The remaining 72 features are aggregated features, obtained using transaction information one-hop backward/forward from the center node - giving the maximum, minimum, standard deviation and correlation coefficients of the neighbour transactions for the same information data (number of inputs/outputs, transaction fee, etc.).\n",
    "\n",
    "### Dataset files\n",
    "\n",
    "The dataset consists of three files:\n",
    "* **elliptic_txs_classes.csv:** Each node is labelled as a \"licit\" (0), \"illicit\" (1), or \"unkonwn\" (2) entity in the class column, the txId column is a unique identifier to the node.  \n",
    "* **elliptic_txs_edgelist.csv:** A list of nodes who are connected. The file has two columns txID1 and txId2. \n",
    "* **elliptic_txs_features.csv:** A file with 171 columns with the first column the transaction id, and the other columns node features. \n",
    "\n",
    "For detailed statistics, please visit the Kaggle Data Explorer of the [Elliptic Data Set](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "We use the FinTorch.datasets library to load the [Elliptic Data Set](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set). The following code downloads the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# from fintorch.datasets import elliptic\n",
    "from fintorch.datasets import elliptic\n",
    "\n",
    "# Load the elliptic dataset\n",
    "elliptic_dataset = elliptic.EllipticDataset('~/.fintorch_data', force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss the code line by line:\n",
    "\n",
    "1. **Importing:** We import the elliptic module from the fintorch.datasets package. This module provides convenient access to the Elliptic Bitcoin Dataset.\n",
    "\n",
    "2. **Loading the Dataset:** We create an instance of the elliptic.EllipticDataset class and store it in the dataset variable. This loads the dataset from Kaggle and places it in the .fintorch_data/ directory. The fintorch framework uses with the [Kaggle API](https://github.com/Kaggle/kaggle-api) to download datasets. Make sure you've followed the instructions in the fintorch documentation to set up your Kaggle API credentials for seamless data access.\n",
    "\n",
    "\n",
    "\n",
    "With the dataset ready, let's examine its structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the PyTorch DataSet into a Polars DataSet and perform basic exploratory data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fintorch.datasets.elliptic.EllipticDataset"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(elliptic_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a single graph thus we access element 0 in the data list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[203769, 167], edge_index=[2, 234355], y=[203769], train_mask=[203769], val_mask=[203769], test_mask=[203769])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elliptic_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following elements in the dataset:\n",
    "* **x:** 203.769 nodes with 167 feature values\n",
    "* **edge_index:** 234.355 pairs of nodes representing the edges between nodes. Note that we transformed the node names into indices. The mapping is stored in *elliptic_dataset.map_id*\n",
    "* **train_mask:** a mask to indicate which nodes are used to train the model\n",
    "* **val_mask:** a mask to indicate which nodes are used as validation set\n",
    "* **test_mask:** a mask to indicate which nodes are used as a test set\n",
    "\n",
    "In addition, we can query some properties of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of node features: 167\n",
      "Number of edge features: 0\n",
      "Number of classes: 3\n",
      "Feature input matrix shape:torch.Size([203769, 167])\n",
      "Edge index feature matrix shape:torch.Size([2, 234355])\n",
      "Label feature matrix shape:torch.Size([203769])\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of node features: {elliptic_dataset.num_features}')\n",
    "print(f'Number of edge features: {elliptic_dataset.num_edge_features}')\n",
    "print(f'Number of classes: {elliptic_dataset.num_classes}')\n",
    "print(f'Feature input matrix shape:{elliptic_dataset.x.shape}')\n",
    "print(f'Edge index feature matrix shape:{elliptic_dataset.edge_index.shape}')\n",
    "print(f'Label feature matrix shape:{elliptic_dataset.y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 3)\n",
      "┌─────┬────────┬──────────────────┐\n",
      "│     ┆ count  ┆ count_normalized │\n",
      "│ --- ┆ ---    ┆ ---              │\n",
      "│ i64 ┆ u32    ┆ f64              │\n",
      "╞═════╪════════╪══════════════════╡\n",
      "│ 2   ┆ 157205 ┆ 0.771486         │\n",
      "│ 0   ┆ 42019  ┆ 0.206209         │\n",
      "│ 1   ┆ 4545   ┆ 0.022305         │\n",
      "└─────┴────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pol\n",
    "\n",
    "# Convert elliptic_dataset.y to a numpy array and then to a polars Series\n",
    "y_series = pol.Series(elliptic_dataset.y.numpy())\n",
    "\n",
    "# Calculate the fraction of each value in the distribution\n",
    "fraction = y_series.value_counts() \n",
    "# Normalize the count column in fraction\n",
    "fraction = fraction.with_columns(count_normalized = fraction['count'] / y_series.shape[0])\n",
    "\n",
    "\n",
    "# Print the fraction of the value distribution\n",
    "print(fraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution reveals a severe imbalance, with the \"unknown\" class dominating 80% of the dataset. This means that a naive model could achieve a misleadingly high accuracy of 80% simply by always predicting the majority class.  It's crucial to be aware of this imbalance when evaluating model performance on this dataset. \n",
    "Relying solely on accuracy could lead to the false impression that a model is performing well, when in reality it's merely taking advantage of the skewed distribution. \n",
    "To get a true understanding of model performance, consider metrics like precision, recall, and F1-score. \n",
    "Additionally, it's important to explore techniques for addressing class imbalance, such as resampling, cost-sensitive learning, or specialized loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model\n",
    "While we'll demonstrate loading the Elliptic dataset into a simple graph neural network model and use accuracy as the evaluation metric for illustrative purposes, it's important to remember the class imbalance we just discussed. In this case, accuracy alone won't be a reliable measure of performance. Our main focus here is to showcase the loading process, not achieve optimal performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a Graph Neural Network (GNN) model for processing data arranged in graphs. It takes node features and information about how the nodes connect as inputs. The model then stacks several layers that combine features from neighboring nodes, similar to how information spreads in a network. Finally, it outputs a new set of features for each node, potentially useful for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in: int, c_hidden: int, c_out: int, num_layers: int = 5, dp_rate: float = 0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Elliptical class.\n",
    "\n",
    "        Args:\n",
    "            c_in (int): Number of input channels.\n",
    "            c_hidden (int): Number of hidden channels.\n",
    "            c_out (int): Number of output channels.\n",
    "            num_layers (int, optional): Number of GNN layers. Defaults to 5.\n",
    "            dp_rate (float, optional): Dropout rate. Defaults to 0.1.\n",
    "            **kwargs: Additional keyword arguments to be passed to the GNN layers.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        gnn_layer = geom_nn.GCNConv\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for _ in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels,\n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "            edge_index (Tensor): Edge index tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        for l in self.layers:\n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                # In case of a geom layer, also pass the edge_index list\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code builds on the GNN model by turning it into a PyTorch Lightning module for training and evaluation. We define a GNN class that trains the model to predict the class of individual nodes in a graph. During training, we calculate accuracy and loss on a subset of nodes (masks) and update the model weights to minimize the loss. We also track the accuracy on validation and test sets to monitor the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class GNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "\n",
    "        self.model = GNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.model(x, edge_index)\n",
    "\n",
    "        # Get the mask \n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, f\"Unknown forward mode: {mode}\"\n",
    "\n",
    "        # Calculate the loss for the mask\n",
    "        loss = self.loss_module(x[mask], data.y[mask].long())\n",
    "        pred = x[mask].argmax(dim=-1)\n",
    "        \n",
    "        return loss, pred, data.y[mask]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.05)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self.forward(batch, mode=\"train\")\n",
    "\n",
    "        # log step metric\n",
    "        self.accuracy(preds, y)\n",
    "        self.log('train_acc_step', self.accuracy)\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self.forward(batch, mode=\"val\")\n",
    "\n",
    "        # log step metric\n",
    "        self.accuracy(preds, y)\n",
    "        self.log('val_acc_step', self.accuracy)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self.forward(batch, mode=\"test\")\n",
    "\n",
    "        # log step metric\n",
    "        self.accuracy(preds, y)\n",
    "        self.log('test_acc_step', self.accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code sets up the training process for our model. It prepares the graph dataset for training using a DataLoader, creates the GNN model, configures a PyTorch Lightning trainer to manage the training process on a GPU, trains the model, tests its performance on a test set, and finally returns the trained GNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def train_node_classifier(dataset, **model_kwargs):\n",
    "    node_data_loader = DataLoader(dataset, batch_size = 1)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=1000,\n",
    "                         enable_progress_bar=False) # False because epoch size is 1\n",
    "    \n",
    "    # Note: the dimensions are specific for the Elliptic dataset\n",
    "    model = GNN(c_in=167, c_out=3, **model_kwargs)\n",
    "    trainer.fit(model, train_dataloaders=node_data_loader, val_dataloaders=node_data_loader)\n",
    "\n",
    "    # Test best model on the test set\n",
    "    trainer.test(model, node_data_loader, verbose=True)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call the code to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | accuracy    | MulticlassAccuracy | 0     \n",
      "1 | model       | GNNModel           | 241 K \n",
      "2 | loss_module | CrossEntropyLoss   | 0     \n",
      "---------------------------------------------------\n",
      "241 K     Trainable params\n",
      "0         Non-trainable params\n",
      "241 K     Total params\n",
      "0.965     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_acc_step         0.8072921633720398\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "node_gnn_model = train_node_classifier(dataset=elliptic_dataset,\n",
    "                                                        c_hidden=256,\n",
    "                                                        num_layers=5,\n",
    "                                                        dp_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training ran on a GPU and successfully trained your GNN model with 241,000 parameters. It reached the maximum of 1000 epochs. On the test data, the model achieved an accuracy of around 80.7%. Note that this accuracy level is misleading! \n",
    "Let's check what it actually predicts with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1, 3)\n",
      "┌─────┬────────┬──────────────────┐\n",
      "│     ┆ count  ┆ count_normalized │\n",
      "│ --- ┆ ---    ┆ ---              │\n",
      "│ i64 ┆ u32    ┆ f64              │\n",
      "╞═════╪════════╪══════════════════╡\n",
      "│ 2   ┆ 203769 ┆ 1.0              │\n",
      "└─────┴────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import polars as pol\n",
    "\n",
    "output = node_gnn_model.model(elliptic_dataset.x, elliptic_dataset.edge_index)\n",
    "# Assuming your tensor is named 'tensor'\n",
    "argmax_tensor = torch.argmax(output, dim=1)\n",
    "\n",
    "# Convert elliptic_dataset.y to a numpy array and then to a polars Series\n",
    "y_series = pol.Series(argmax_tensor.numpy())\n",
    "\n",
    "# Calculate the fraction of each value in the distribution\n",
    "fraction = y_series.value_counts() \n",
    "# Normalize the count column in fraction\n",
    "fraction = fraction.with_columns(count_normalized = fraction['count'] / y_series.shape[0])\n",
    "\n",
    "# Print the fraction of the value distribution\n",
    "print(fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model only predicts a single class and achieves high levels of accuracy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
