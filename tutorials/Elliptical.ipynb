{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "In this hands-on Python tutorial, we'll delve into an intriguing dataset that captures the dynamics of transactions within a blockchain network. Our goal will be to prepare this data for training machine learning models using the powerful PyTorch and PyTorch Lightning frameworks.\n",
    "\n",
    "# The Dataset\n",
    "\n",
    "Let's dissect the dataset's description:\n",
    "\n",
    "* Temporal Structure: The dataset is divided into 49 distinct time steps, each spaced roughly two weeks apart. Within every time step, we find a connected group of transactions occurring within a three-hour window.\n",
    "* Transaction Features: Each transaction is characterized by 94 'local' features. These include its timestamp, input/output counts, fees, volume, and interesting aggregations (e.g., average BTC involved in inputs/outputs).\n",
    "* Neighborhood Features: An additional 72 'aggregated' features illuminate each transaction's context. We get statistics like the maximum, minimum, standard deviation, and correlation coefficients derived from transactions one hop away.\n",
    "\n",
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "We'll begin our journey with exploratory data analysis (EDA). Key things to explore:\n",
    "\n",
    "* Distributions: Examine the distributions of transaction features (fees, volumes, etc.) to spot patterns and potential outliers.\n",
    "* Correlations: Investigate relationships between transaction features. Which features correlate, and can this insight inform our model design?\n",
    "* Temporal Trends: Analyze how features change across the 49 time steps. Are there seasonal effects or evolving network behaviors?\n",
    "\n",
    "# PyTorch Datasets\n",
    "\n",
    "Our EDA findings will guide how we structure our PyTorch Datasets. Here's where things get exciting:\n",
    "\n",
    "* Custom Dataset Class: We'll create a custom PyTorch Dataset class to load and preprocess the raw data dynamically during model training.\n",
    "* Data Transformations: We might apply scaling, normalization, or other essential transformations to make the data more suitable for machine learning.\n",
    "\n",
    "# PyTorch Lightning Integration\n",
    "\n",
    "Finally, we'll leverage PyTorch Lightning to streamline our training process.\n",
    "\n",
    "* DataModule: A PyTorch Lightning DataModule will encapsulate our Datasets, manage data loading, and handle batching for efficient model training.\n",
    "\n",
    "# What You'll Build\n",
    "\n",
    "By the end of this tutorial, you'll have a solid foundation for training machine learning models on this dataset. This foundation sets the stage for exciting applications such as:\n",
    "\n",
    "* Fraud detection\n",
    "* Transaction pattern analysis\n",
    "* Blockchain network behavior prediction\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset background\n",
    "The dataset description originates from [Kaggle Elliptic Data Set](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set) and is restated here for convenience. \n",
    "\n",
    "## Dataset description\n",
    "This anonymized data set is a transaction graph collected from the Bitcoin blockchain. A node in the graph represents a transaction, an edge can be viewed as a flow of Bitcoins between one transaction and the other. Each node has 166 features and has been labeled as being created by a \"licit\", \"illicit\" or \"unknown\" entity.\n",
    "\n",
    "### Nodes and edges\n",
    "\n",
    "The graph is made of 203,769 nodes and 234,355 edges. Two percent (4,545) of the nodes are labelled class1 (illicit). Twenty-one percent (42,019) are labelled class2 (licit). The remaining transactions are not labelled with regard to licit versus illicit.\n",
    "\n",
    "### Features\n",
    "\n",
    "There are 166 features associated with each node. Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset. There is a time step associated to each node, representing a measure of the time when a transaction was broadcasted to the Bitcoin network. The time steps, running from 1 to 49, are evenly spaced with an interval of about two weeks. Each time step contains a single connected component of transactions that appeared on the blockchain within less than three hours between each other; there are no edges connecting the different time steps.\n",
    "\n",
    "The first 94 features represent local information about the transaction â€“ including the time step described above, number of inputs/outputs, transaction fee, output volume and aggregated figures such as average BTC received (spent) by the inputs/outputs and average number of incoming (outgoing) transactions associated with the inputs/outputs. The remaining 72 features are aggregated features, obtained using transaction information one-hop backward/forward from the center node - giving the maximum, minimum, standard deviation and correlation coefficients of the neighbour transactions for the same information data (number of inputs/outputs, transaction fee, etc.).\n",
    "\n",
    "### Dataset files\n",
    "\n",
    "The dataset consists of three files:\n",
    "* **elliptic_txs_classes.csv:** Each node is labelled as a \"licit\" (0), \"illicit\" (1), or \"unkonwn\" (2) entity in the class column, the txId column is a unique identifier to the node.  \n",
    "* **elliptic_txs_edgelist.csv:** A list of nodes who are connected. The file has two columns txID1 and txId2. \n",
    "* **elliptic_txs_features.csv:** A file with 171 columns with the first column the transaction id, and the other columns node features. \n",
    "\n",
    "For detailed statistics, please visit the Kaggle Data Explorer of the [Elliptic Data Set](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "We use the FinTorch.datasets library to load the [Elliptic Data Set](https://www.kaggle.com/datasets/ellipticco/elliptic-data-set). The following code downloads the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fintorch.datasets import elliptic\n",
    "from fintorch.datasets import elliptic\n",
    "\n",
    "# Load the elliptic dataset\n",
    "elliptic_dataset = elliptic.EllipticDataset('~/.fintorch_data', force_reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss the code line by line:\n",
    "\n",
    "1. **Importing:** We import the elliptic module from the fintorch.datasets package. This module provides convenient access to the Elliptic Bitcoin Dataset.\n",
    "\n",
    "2. **Loading the Dataset:** We create an instance of the elliptic.EllipticDataset class and store it in the dataset variable. This loads the dataset from Kaggle and places it in the .fintorch_data/ directory. The fintorch framework uses with the [Kaggle API](https://github.com/Kaggle/kaggle-api) to download datasets. Make sure you've followed the instructions in the fintorch documentation to set up your Kaggle API credentials for seamless data access.\n",
    "\n",
    "\n",
    "\n",
    "With the dataset ready, let's examine its structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the PyTorch DataSet into a Polars DataSet and perform basic exploratory data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(elliptic_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a single graph thus we access element 0 in the data list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elliptic_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following elements in the dataset:\n",
    "* **x:** 203.769 nodes with 167 feature values\n",
    "* **edge_index:** 234.355 pairs of nodes representing the edges between nodes. Note that we transformed the node names into indices. The mapping is stored in *elliptic_dataset.map_id*\n",
    "* **train_mask:** a mask to indicate which nodes are used to train the model\n",
    "* **val_mask:** a mask to indicate which nodes are used as validation set\n",
    "* **test_mask:** a mask to indicate which nodes are used as a test set\n",
    "\n",
    "In addition, we can query some properties of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of node features: {elliptic_dataset.num_features}')\n",
    "print(f'Number of edge features: {elliptic_dataset.num_edge_features}')\n",
    "print(f'Number of classes: {elliptic_dataset.num_classes}')\n",
    "print(f'Feature input matrix shape:{elliptic_dataset.x.shape}')\n",
    "print(f'Edge index feature matrix shape:{elliptic_dataset.edge_index.shape}')\n",
    "print(f'Label feature matrix shape:{elliptic_dataset.y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pol\n",
    "\n",
    "# Convert elliptic_dataset.y to a numpy array and then to a polars Series\n",
    "y_series = pol.Series(elliptic_dataset.y.numpy())\n",
    "\n",
    "# Calculate the fraction of each value in the distribution\n",
    "fraction = y_series.value_counts() \n",
    "# Normalize the count column in fraction\n",
    "fraction = fraction.with_columns(count_normalized = fraction['count'] / y_series.shape[0])\n",
    "\n",
    "\n",
    "# Print the fraction of the value distribution\n",
    "print(fraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the distribution of the output class and we observe that roughly 80% has label unknown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model\n",
    "A model to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.data as geom_data\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "gnn_layer_by_name = {\n",
    "    \"GCN\": geom_nn.GCNConv,\n",
    "    \"GAT\": geom_nn.GATConv,\n",
    "    \"GraphConv\": geom_nn.GraphConv\n",
    "}\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of \"hidden\" graph layers\n",
    "            layer_name - String of the graph layer to use\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        gnn_layer = gnn_layer_by_name[layer_name]\n",
    "\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels,\n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "\n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of hidden layers\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                nn.Linear(in_channels, out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [nn.Linear(in_channels, c_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeLevelGNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        if model_name == \"MLP\":\n",
    "            self.model = MLPModel(**model_kwargs)\n",
    "        else:\n",
    "            self.model = GNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.model(x, edge_index)\n",
    "\n",
    "        # Only calculate the loss on the nodes corresponding to the mask\n",
    "        if mode == \"train\":\n",
    "            mask = data.train_mask\n",
    "        elif mode == \"val\":\n",
    "            mask = data.val_mask\n",
    "        elif mode == \"test\":\n",
    "            mask = data.test_mask\n",
    "        else:\n",
    "            assert False, f\"Unknown forward mode: {mode}\"\n",
    "\n",
    "\n",
    "        loss = self.loss_module(x[mask], data.y[mask].long())\n",
    "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
    "        return loss, acc\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # We use SGD here, but Adam works as well\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = \"./logs\"\n",
    "\n",
    "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
    "    # pl.seed_everything(42)\n",
    "    node_data_loader = DataLoader(dataset, batch_size = 1)\n",
    "\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         accelerator=\"gpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=00,\n",
    "                         enable_progress_bar=False) # False because epoch size is 1\n",
    "    \n",
    "    # pl.seed_everything()\n",
    "    model = NodeLevelGNN(model_name=model_name, c_in=167, c_out=3, **model_kwargs)\n",
    "    trainer.fit(model, node_data_loader, node_data_loader)\n",
    "    model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on the test set\n",
    "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
    "    batch = next(iter(node_data_loader))\n",
    "    batch = batch.to(model.device)\n",
    "    _, train_acc = model.forward(batch, mode=\"train\")\n",
    "    _, val_acc = model.forward(batch, mode=\"val\")\n",
    "    result = {\"train\": train_acc,\n",
    "              \"val\": val_acc,\n",
    "              \"test\": test_result[0]['test_acc']}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small function for printing the test scores\n",
    "def print_results(result_dict):\n",
    "    if \"train\" in result_dict:\n",
    "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
    "    if \"val\" in result_dict:\n",
    "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
    "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_mlp_model, node_mlp_result = train_node_classifier(model_name=\"MLP\",\n",
    "                                                        dataset=elliptic_dataset,\n",
    "                                                        c_hidden=16,\n",
    "                                                        num_layers=4,\n",
    "                                                        dp_rate=0.1)\n",
    "\n",
    "print_results(node_mlp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import polars as pol\n",
    "\n",
    "output = node_mlp_model.model(elliptic_dataset.x, elliptic_dataset.edge_index)\n",
    "# Assuming your tensor is named 'tensor'\n",
    "argmax_tensor = torch.argmax(output, dim=1)\n",
    "\n",
    "# Convert elliptic_dataset.y to a numpy array and then to a polars Series\n",
    "y_series = pol.Series(argmax_tensor.numpy())\n",
    "\n",
    "# Calculate the fraction of each value in the distribution\n",
    "fraction = y_series.value_counts() \n",
    "# Normalize the count column in fraction\n",
    "fraction = fraction.with_columns(count_normalized = fraction['count'] / y_series.shape[0])\n",
    "\n",
    "# Print the fraction of the value distribution\n",
    "print(fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_gnn_model, node_gnn_result = train_node_classifier(model_name=\"GNN\",\n",
    "                                                        layer_name=\"GCN\",\n",
    "                                                        dataset=elliptic_dataset,\n",
    "                                                        c_hidden=256,\n",
    "                                                        num_layers=5,\n",
    "                                                        dp_rate=0.1)\n",
    "print_results(node_gnn_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
